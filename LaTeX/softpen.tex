\synctex=1
\documentclass[11pt, a4paper]{article}

%% Packages
\usepackage{subcaption}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{hyperref}
% \usepackage{fullpage}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{color}
\usepackage[left = 2.5cm, right = 2.5cm, bottom = 3cm, top = 2.5cm]{geometry}
% \usepackage{enumitem}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{calc}
\usepackage{bm}
\usepackage{pdfpages}
\usepackage{authblk}
\usepackage{dcolumn}
\usepackage{float}


%\usepackage[inline]{showlabels}
%\renewcommand{\showlabelfont}{\small\tt\color{red}}

%% Commands 
% \newcommand*{\bb}{}
\newcommand*{\bb}{\boldsymbol}
\newcommand{\IK}[1]{{\noindent \color{blue} \bf \#IK: #1}}
\newcommand{\PS}[1]{{\noindent \color{red} \bf \#PS: #1}}
\newcommand{\iq}[3]{#1^{#2}_{#3}}
\newcommand{\mi}[5]{\prescript{#3}{#2}{{#1}}_{#4}^{#5}}
\newcommand{\Q}[4]{\tilde Q_{#1}^{(#2,#3,#4)}}
\newcommand{\pQ}[4]{Q_{#1}^{(#2,#3,#4)}}
\newcommand{\Op}[1]{\ensuremath{{\mathcal{O}_p(#1)}}}
\newcommand{\op}[1]{\ensuremath{{o_p(#1)}}}


\newcommand{\vnorm}[1]{\ensuremath{{\left\| #1 \right\|}}}
\newcommand{\mnorm}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
		\right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
\newcommand{\mnorms}[1]{{\vert\kern-0.25ex\vert\kern-0.25ex\vert #1 
		\vert\kern-0.25ex\vert\kern-0.25ex\vert}}

       
%% Theorems etc
\newtheoremstyle{example}% name
{3pt} %Space above
{3pt} %Space below
{} %Body font
{0\parindent} %Indent amount 1
{\bf}
% {\scshape} %Theorem head font
{:} %Punctuation after theorem head
{.5em} %Space after theorem head 2
{} %Theorem head spec (
\newtheoremstyle{theorem}% name
{3pt} %Space above
{3pt} %Space below
{\em} %Body font
{0\parindent} %Indent amount 1
{\bf}
% {\scshape} %Theorem head font
{:} %Punctuation after theorem head
{.5em} %Space after theorem head 2
{} %Theorem head spec (
\theoremstyle{example} \newtheorem{example}{Example}[section]
\theoremstyle{theorem} \newtheorem{theorem}{Theorem}[section]
\newtheorem*{proposition}{Proposition}
\newtheorem{lemma}[theorem]{Lemma}


%% Definitions
\def\sign{\mathop{\rm sign}}
\def\expect{{\mathop{\rm E}}}
\def\var{{\mathop{\rm var}}}
\def\cov{{\mathop{\rm cov}}}
\def\trace{{\mathop{\rm trace}}}
\def\det{{\mathop{\rm det}}}

\def\bbeta{\bb{\beta}}
\def\btheta{\bb{\theta}}
\def\bgamma{\bb{\gamma}}
\def\bSigma{\bb{\Sigma}}
\def\bgamma{\bb{\gamma}}
\def\by{\bb{y}}
\def\bC{\bb{C}}
\def\bY{\bb{Y}}
\def\bu{\bb{u}}
\def\bx{\bb{x}}
\def\bz{\bb{z}}
\def\b0{\bb{0}}
\def\bX{\bb{X}}
\def\bZ{\bb{Z}}
\def\bv{\bb{v}}
\def\bV{\bb{V}}
\def\bY{\bb{Y}}
\def\by{\bb{y}}
\def\bL{\bb{L}}
\def\bLt{\tilde{\bb{L}}}
\def\btnod{\bb{\theta}_0}
\def\bttilde{\tilde{\bb{\theta}}}
\def\bW {\bb{W}}
%% Title Page 
\title{Maximum softly-penalized likelihood for Bernoulli-response generalized linear mixed models}
 
\author[1]{Philipp Sterzinger}
\author[1,2]{Ioannis Kosmidis}

\affil[1]{Department of Statistics, University of Warwick, Coventry, CV4 7AL, UK}
\affil[2]{The Alan Turing Institute, London, NW1 2DB, UK}


\begin{document}
\maketitle
%\tableofcontents

\begin{abstract}
  We introduce a soft penalization approach for stabilizing maximum
  likelihood estimation in Bernoulli-response generalized linear mixed
  models, which is known to have a positive probability to result in
  estimates on the boundary of the parameter space. Such estimates, instances of which
  are infinite values for fixed effects and singular or
  infinite variance components, can cause havoc to numerical
  estimation procedures and inference. We introduce an additive penalty to the log-likelihood function, 
  which consists of appropriately scaled versions of the Jeffreys
  prior for the model with no random effects and the negative Huber loss. The resulting maximum softly-penalized likelihood
  estimates are shown to lie in the interior of the parameter
  space. Appropriate scaling of the penalty guarantees that the penalization is soft-enough to recover
  the optimal asymptotic properties expected by the maximum likelihood
  estimator, namely consistency, asymptotic normality,
  Cram\'{e}r-Rao efficiency and asymptotically valid hypothesis testing. Further, our choice of penalties and scaling factor preserves invariance of the fixed effects estimates under linear transformations of the model
  parameters, such as contrasts. Maximum softly-penalized likelihood
  is compared to competing approaches on two real-data examples,
  and comprehensive simulation studies that illustrate its superior finite sample
  performance.
  \bigskip \\
  \noindent {Keywords: logistic regression, infinite estimates, singular variance components, data separation, Jeffreys prior}
\end{abstract}

\section{Introduction}
\label{sec:intro}

Generalized Linear Mixed Models (GLMMs; \citealt[Chapter
7]{mcculloch+etal:2008}) are a potent class of statistical models
that allow associating Gaussian and non-Gaussian responses, such as
counts, proportions, positive responses, and so on, with covariates,
while accounting for complex multivariate dependencies. This is achieved by linking
the expectation of a response to a linear combination of covariates
and parameters (fixed effects), and sources of extra variation (random
effects) with known distributions. Although these models find
application in numerous fields such as biology, ecology and the social
sciences \citep{bolker+etal:2009}, estimation of GLMMs is not
straightforward in practice, because their likelihood is generally an intractable
multivariate integral.

Maximum approximate likelihood (MAL) methods maximize an 
approximation of the GLMM likelihood, that can, in principle, be
chosen to be arbitrarily accurate \citep[see, for example,
][]{raudenbush+etal:2000, pinheiro+chao:2006}. Such methods are
pervasive in contemporary GLMM practice because, like maximum
likelihood (ML), MAL estimators are consistent under general conditions
about the model, and the MAL estimates and the approximate likelihood
itself can be used for the construction of likelihood-based
inferences, such as likelihood-ratio tests or Wald
statistics, and can be used for model selection based on information criteria. An
alternative approach to MAL are Bayesian posterior update procedures
\citep[see, for example,][]{zhao+etal:2006,browne+draper:2006}. However, they come with
various technical difficulties, such as determining the scaling of the
covariates, selecting appropriate priors, coming up with efficient
posterior sampling algorithms, and determining burn-in times of chains
for reliable estimation. Yet another alternative to MAL are maximum
penalized quasi-likelihood (MPQL) methods \citep{schall:1991,
  wolfinger+oconnel:1993, breslow+clayton:1993} which essentially fit a Linear Mixed Model to transformed pseudo-responses. However, the penalized quasi likelihood may not
yield an accurate approximation of the GLMM likelihood. As a result,
MPQL estimators can have large bias when the random effects
variances are large
\citep{bolker+etal:2009,rodriguez+goldman1995} and are
not necessarily consistent \citep[Chapter 3.1]{jiang:2017}.

Despite the pervasiveness of MAL, certain data configurations can
result in MAL estimates of the variance-covariance matrix of the
random effects distribution to be on the boundary of the parameter
space, such as infinite or zero estimated variances, or, more
generally, singular estimates of the variance-covariance matrix; see
\cite{chung+etal:2013} for an excellent discussion. In
addition, as is the case in maximum likelihood estimation of
Bernoulli-response generalized linear models \citep[GLMs; see, for
example][Chapter 4]{mccullagh+nelder:1989}, the MAL estimates of the
fixed effects can be
infinite. As is well-acknowledged in the GLMM literature \citep[see,
for example][]{bolker+etal:2009, bolker:2018,
  pasch+etal:2013}, both instances of boundary estimates can
cause havoc to numerical optimization procedures used for MAL. In
addition, if they go undetected, they can substantially impact
first-order inferential procedures, like Wald tests, resulting in
spuriously strong or weak conclusions. In contrast to the
numerous approaches to detect (see, for example,
\citealt{kosmidis+schumacher:2021} for the \texttt{detectseparation} R
package that implements the methods in \citealt{konis:2017}) and
handle \citep[see, for example,][]{kosmidis+firth:2020,
 heinze+schemper:2002, gelman+etal:2008} infinite
estimates in Bernoulli-response GLMs, little methodology or guidance
is available on how to detect or deal with degenerate estimates in
GLMMs.

We introduce a maximum softly-penalized approximate likelihood (MSPAL)
procedure for Bernoulli-response GLMMs that returns estimators that
are guaranteed to take values in the interior of the parameter space,
and are also consistent, asymptotically normal, Cram\'{e}r-Rao
efficient and give asymptotically valid inference, under no additional assumptions beyond those typically
required for establishing consistency, asymptotic normality, and asymptotically valid inference of MAL
or ML estimators. Although the developments here are for
Bernoulli-response GLMMs, they provide a blueprint for the
construction of penalties and estimators with values in the interior
of the parameter space for any GLMM and, more generally, for
M-estimation settings where boundary estimates occur. The
(approximate) likelihood penalty we introduce consists of
appropriately scaled versions of the Jeffreys prior for the model with
no random effects, and the negative Huber loss. We show that the MSPAL
estimates are guaranteed to be in the interior of the parameter space,
and impose a scaling to the penalty
that guarantees that i) penalization is soft-enough for the MSPAL
estimator to have the optimal asymptotic properties expected by the ML
estimator, and ii) that the fixed effects estimates are invariant to linear
transformation of the model parameters, such as contrasts, in the
sense that the MSPAL estimates of linear transformations of the fixed
effects parameters are the linear transformations of the MSPAL
estimates. Both i) and ii) are in contrast to other penalization
procedures that have been proposed in the literature \citep[see, for
example,][]{chung+etal:2013, chung+etal:2015}. Maximum
softly-penalized likelihood is compared to prominent competing
approaches through two real-data examples, and comprehensive
simulation studies that illustrate its superior finite-sample
performance. 

The remainder of the paper is organized as follows. Section \ref{sec:bern_GLMMs} defines the clustered
Bernoulli-response GLMM and Section \ref{sec:culcita_dat} gives a motivating real-data example of
degenerate maximum approximate likelihood estimates in a Bernoulli-response GLMM. Section
\ref{sec:softpen} formalizes the maximum softly penalized approximate likelihood framework and
states the large sample results of the softly penalized maximum
likelihood estimator. Section \ref{sec:ci} demonstrates the performance of the MSPAL on another real-data example and a data-based simulation and Section \ref{sec:sum} provides concluding remarks. Proofs, details on the simulations in this paper and further simulations on synthetic data are given in the supplementary material. 

\section{Bernoulli-response generalized linear mixed models}
\label{sec:bern_GLMMs}

Suppose that response vectors $\by_1, \ldots, \by_k$ are observed with
$\by_i = (y_{i1}, \ldots, y_{in_i})^\top \in \{0, 1\}^{n_i}$, possibly
along with covariate matrices $\bV_1, \ldots, \bV_k$, respectively,
where $\bV_i$ is a $n_i \times s$ matrix.

A Bernoulli-response GLMM assumes that $\by_1, \ldots, \by_k$
are realizations of random vectors $\bY_1, \ldots, \bY_k$, whose entries
$Y_{i1}, \ldots, Y_{in_i}$ $(i = 1, \ldots, k)$ are independent
Bernoulli random variables conditionally on a vector of random effects
$\bu_i$. The vectors $\bu_1, \ldots, \bu_k$ are assumed to be
independent realizations of a multivariate normal distribution, and the conditional mean of each Bernoulli random
variable is linked to a linear predictor $\eta_{ij}$, which is a
linear combination of covariates with random effects and fixed effects.
Specifically,
\begin{align}
\label{eq:bern_cluster}
  Y_{ij} \mid \bb{u}_i & \sim \text{Bernoulli}(\mu_{ij}) \quad \text{with} \quad
  g(\mu_{ij}) = \eta_{ij} = \bx_{ij}^\top \bbeta + \bz_{ij}^\top \bu_i\\
  \bu_i & \sim \text{N}(\b0_q, \bb{\Sigma})  \quad (i = 1, \ldots, k; j = 1, \ldots, n_i)\,,
\end{align}
where $\mu_{ij} = P(Y_{ij} = 1 \mid \bu_i, \bx_{ij}, \bz_{ij})$, and
$g: (0, 1) \to \Re$ is a known monotone increasing link function, like
the logistic, probit or complementary log-log. The vector $\bx_{ij}$
is the $j$th row of the $n_i \times p$ model matrix $\bX_{i}$
associated with the $p$-vector of fixed effects
$\bbeta \in \Re^p$, and $\bz_{ij}$ is the $j$th row of the
$n_i \times q$ model matrix $\bZ_{i}$ associated with the $q$-vector
of random effects $\bu_i$. The model matrices $\bX_i$ and $\bZ_i$ are
formed from subsets of columns of $\bV_i$. The variance-covariance
matrix $\bSigma$ collects the variance components and is assumed to be
symmetric and positive definite. The Bernoulli-response GLMM
in~(\ref{eq:bern_cluster}) is here introduced explicitly in terms of
clusters. The other often encountered formulation of GLMMs in the literature \citep[Chapter 7.4]{mcculloch+etal:2008} absorbs the clustering into the variance components structure of $\bSigma$ and is therefore a clustered GLMM with a single cluster. Hence, the presentation and results here are with no loss
of generality.

The marginal likelihood about $\bb{\beta}$ and $\bb{\Sigma}$ for
model~(\ref{eq:bern_cluster}) is
\begin{equation}
\label{eq:bern_likl}
L(\bbeta,\bSigma) = (2\pi)^{-kq/2} \det(\bSigma)^{-k/2} \prod_{i=1}^{k}\int_{\Re^q}\prod_{j=1}^{n_i} \mu_{ij}^{y_{ij}}(1-\mu_{ij})^{1-y_{ij}} \exp\left\{-\frac{\bu_i^\top\bSigma^{-1}\bu_i}{2} \right\} d\bu_i\,,
\end{equation}
Formally, the ML estimator is the maximizer of~(\ref{eq:bern_likl})
with respect to $\bbeta$ and $\bSigma$. However,
(\ref{eq:bern_likl}) involves intractable integrals, which are
typically approximated before maximization, resulting in MAL
estimators. For example, the popular \texttt{glmer} routine of the
R \citep{R} package \texttt{lme4} \citep{bates+etal:2015} uses adaptive
Gauss-Hermite quadrature for one-dimensional random effects and
Laplace approximation for higher-dimensional random effects. A
detailed account of those approximation methods can be found in
\citet{pinheiro+bates:1995}.

\newpage
\section{Motivating example}
\label{sec:culcita_dat}

The working data set in this section is a reduced version of the data
in \citet{mckeon+etal:2012}, as provided in the worked examples of
\citet{bolker:2015} (available at
\url{https://bbolker.github.io/mixedmodels-misc/ecostats\_chap.html}). The
data is given in Table~\ref{tab:culcita} and comes from trials
involving coral-eating sea stars Culcita novauguineae (hereafter
Culcita) attacking coral that harbour differing combinations of
protective symbionts, involving crabs and shrimp. The design is a
randomised complete block design with two replications per treatment
per block, four treatments, involving no symbionts, crabs only, shrimp
only, both crabs and shrimp, and ten temporal blocks. As a result
there is a total of 80 observations on whether predation was present
(recorded as one) or not (recorded as zero). By mere inspection
of Table~\ref{tab:culcita}, we note that predation becomes more prevalent
with increasing block number, and that predation gets suppressed when either
crabs or shrimp are present, and more so when both symbionts are
present. The only observation that deviates from this general trend is the observation in block 10 with no predation and no symbionts.

  \begin{table}[t]
    \caption{Culcita data \citep{mckeon+etal:2012} from the worked
      examples of \citet{bolker:2015} (available at
      \url{https://bbolker.github.io/mixedmodels-misc/ecostats\_chap.html}).}
    \label{tab:culcita}
    \centering
    \begin{tabular}{lllllllllll}
      \toprule
      & \multicolumn{10}{c}{Block} \\ \cmidrule{2-11}
      \multicolumn{1}{c}{Treatment} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{9} & \multicolumn{1}{c}{10} \\
      \midrule
      none & 0,1 & 1,1 & 1,1 & 1,1 & 1,1 & 1,1 & 1,1 & 1,1 & 1,1 & 1,0 \\
      crabs & 0,0 & 0,0 & 0,0 & 0,0 & 1,1 & 1,1 & 1,1 & 1,1 & 1,1 & 1,1 \\
      shrimp & 0,0 & 0,0 & 0,0 & 0,0 & 0,1 & 1,1 & 1,1 & 1,1 & 1,1 & 1,1 \\
      both & 0,0 & 0,0 & 0,0 & 0,0 & 0,0 & 0,1 & 1,1 & 1,1 & 1,1 & 1,1 \\
      \bottomrule
    \end{tabular}
\end{table}

A Bernoulli-response GLMM with one random intercept per block can be
used here to associate predation to treatment effects while
accounting for heterogeneity between blocks. Such a model can be
defined as
\begin{align}
  \label{eq:logistic_normal}
  Y_{ij} \mid u_i & \sim \text{Bernoulli}(\mu_{ij}) \quad  \text{with} \quad
  \log{\frac{\mu_{ij}}{1 - \mu_{ij}}} =  \eta_{ij} =  \beta_0 + u_i + \beta_{j} \\ 
  u_i & \sim \text{N}(0, \sigma^2) \quad (i = 1, \ldots, 10; j = 1, \ldots, 4)\,.
\end{align}
In the above expressions, $Y_{i1}$, $Y_{i2}$, $Y_{i3}$, and $Y_{i4}$
correspond to responses for ``none'', ``crabs'', ``shrimp'', ``both'',
respectively, and we set $\beta_1 = 0$ for identifiability purposes,
effectively using ``none'' as a reference category. The logarithm of the model likelihood~(\ref{eq:bern_likl}) about the parameters
$\bbeta = (\beta_0, \beta_2, \beta_3, \beta_4)^\top$ and
$\psi = \log\sigma$ for model~(\ref{eq:logistic_normal}) is
approximated using an adaptive quadrature rule with $Q = 100$ points
\citep[see, for example,][]{liu+pierce:1994, pinheiro+bates:1995} as
implemented in \texttt{glmer}.  The approximation to the log-likelihood
gets more accurate as the number of quadrature points increases, and
here we choose $Q = 100$ points, which is the maximum possible in the
current \texttt{glmer} implementation.

All parameter estimates of model~(\ref{eq:logistic_normal})
reported in the current example are computed after removing the
atypical observation with zero predation in block 10 when there are no
symbionts. Estimates based on all data points are provided in
Table~\ref{XXX} of the supplementary materials \IK{include that}.

The MAL estimates of $\bbeta$ and $\psi$ in
Table~\ref{tab:culcita_inf} are computed using the numerical
optimization procedures ``BFGS'' and ``CG'' (MAL(BFGS) and MAL(CG),
respectively), as these are readily available from the \texttt{optimx} R
package \citep[see][Section 3 for details]{nash+varadhan:2011}, with
default starting values. The MAL(BFGS) and MAL(CG) estimates are different,
and are notably extreme on the logistic scale. This is due to the two
optimization procedures stopping early at different points in the
parameter space after having prematurely declared convergence. The large estimated
standard errors are indicative of the approximation
to the log-likelihood being almost flat around the estimates. In this
case, the MAL estimates for the fixed effects
$\beta_0, \beta_1, \beta_2, \beta_3$ are in reality infinite in
absolute value.

\begin{table}[t]
  \caption{Estimates from the degenerate Culcita subdataset of \citet{bolker:2015} using MAL,MSPAL and \texttt{bglmer} } 
  \label{tab:culcita_inf}
  \centering
  \begin{tabular}{lD{.}{.}{3}D{.}{.}{3}D{.}{.}{3}D{.}{.}{3}D{.}{.}{3}}
    \toprule
%     &
% \multicolumn{2}{c}{MAL} &
% \multicolumn{2}{c}{bglmer} & 
% \multicolumn{1}{c}{MSPAL} \\ \cmidrule{2-3}\cmidrule{4-5}
&
\multicolumn{1}{c}{MAL(BFGS)} & 
\multicolumn{1}{c}{MAL(CG)} &
\multicolumn{1}{c}{bglmer(t)} &
\multicolumn{1}{c}{bglmer(n)} & \multicolumn{1}{c}{MSPAL}
    \\
\midrule
\multicolumn{6}{c}{reference category: ``none''} \\
\midrule
$\beta_0$ & 15.88 & 15.37 & 6.39 & 4.90 & 8.41\\
            & (10.14) & (9.50) & (2.60) & (2.08) & (3.43)\\
$\beta_2$    & -12.93 & -12.46 & -4.02 & -2.84 & -7.22\\
            & (9.15) & (8.53) & (1.59) & (1.27) & (3.21)\\
$\beta_3$   & -14.81 & -14.30 & -4.81 & -3.44 & -8.26\\
            & (9.89) & (9.24) & (1.73) & (1.35) & (3.48)\\
$\beta_4$     & -17.71 & -17.15 & -6.47 & -4.73 & -10.10\\
            & (10.70) & (10.02) & (2.05) & (1.57) & (3.84)\\
$\log\sigma$   & 2.31 & 2.28 & 1.72 & 1.54 & 1.80\\
            & (0.64) & (0.62) & (0.44) & (0.43) & (0.45)\\
\midrule
\multicolumn{6}{c}{reference category: ``both''} \\
\midrule
$\gamma_0$ & -1.82 & -1.74 & 0.37 & 0.57 & -1.70\\
            & (3.92) & (3.77) & (2.24) & (2.07) & (2.46)\\
$\gamma_1$    & 17.74 & 17.09 & 6.70 & 5.75 & 10.10\\
            & (10.75) & (10.03) & (2.19) & (1.88) & (3.84)\\
$\gamma_2$   & 4.78 & 4.65 & 1.63 & 1.26 & 2.88\\
            & (3.08) & (2.98) & (1.43) & (1.32) & (1.85)\\
$\gamma_3$   & 2.89 & 2.83 & 0.83 & 0.56 & 1.85\\
            & (2.27) & (2.22) & (1.35) & (1.28) & (1.60)\\
$\log\sigma$   & 2.31 & 2.28 & 1.74 & 1.66 & 1.80\\
            & (0.64) & (0.62) & (0.44) & (0.44) & (0.45)\\
\bottomrule
  \end{tabular}
\end{table}


Parameter estimates are also obtained using the \texttt{bglmer}
routine of the \texttt{blme} R package \citep{chung+etal:2013} that has
been developed to ensure that parameter estimates from GLMMs are away
from the boundary of the parameter space. The estimates shown in
Table~\ref{tab:culcita_inf} are obtained using a penalty for $\sigma$
inspired by a gamma prior (default in \texttt{bglmer}; see
\citealt{chung+etal:2013} for details) and two of the default prior
specifications for the fixed effects: i) independent normal priors
(``bglmer(n)''), and ii) independent t priors (``bglmer(t)''), as
these are implemented in \texttt{blme}; see \texttt{bmerDist-class} in
the help pages of \texttt{blme} for details. We also show the
estimates obtained using the MSPAL estimation method that we propose
in the current work.

The maximum penalized approximate likelihood estimates from
\texttt{bglmer} and the corresponding estimated standard errors appear to
be finite. Nevertheless, the use of the default priors directly breaks
parametrization invariance under contrasts, which MAL estimates
enjoy. For example, Table~\ref{tab:culcita_inf} also shows the
estimates of model~(\ref{eq:logistic_normal}) with
$\eta_{ij} = \gamma_0 + u_i + \gamma_{j}$, where $\gamma_{4} = 0$,
i.e.~setting ``both'' as a reference category. Hence, the identities
$\gamma_0 = \beta_0 + \beta_4$, $\gamma_1 = -\beta_4$,
$\gamma_2 = \beta_2 - \beta_4$, $\gamma_3 = \beta_3 - \beta_4$ hold,
and it is natural to expect those identities from the estimates of
$\bbeta$ and $\bgamma$. As is evident from
Table~\ref{tab:culcita_inf}, the \texttt{bglmer} estimates with either
normal or t priors can deviate substantially from those
identities. For example, the \texttt{bglmer} estimate of $\gamma_1$
based on normal priors is $5.75$ while that for $\beta_4$ is $-4.73$,
and the estimate of $\log\sigma$ is $1.54$ in the $\bbeta$
parametrization and $1.66$ in the $\bgamma$
parametrization. Furthermore, different contrasts give varying
amounts of deviations from these identities. On the other hand, the
approximate likelihood is invariant to monotone parameter
transformations. As a result, the corresponding identities hold
exactly for the MAL estimates with the deviations observed in
Table~\ref{tab:culcita_inf} being due to early stopping of the optimization routines.

The \texttt{bglmer} estimates are typically closer to zero in absolute
value than the MAL estimates because the normal and t priors are all
centred at zero. Furthermore, the estimates using normal priors tend
to shrink more towards zero than those using t priors, because the
latter have heavier tails than the former. In order to assess the
impact of shrinkage on the frequentist properties of the estimators,
we simulate $10000$ independent samples of responses for the randomized
complete block design in Table~\ref{tab:culcita}, at the MAL estimates
in the $\bbeta$ parametrization when all data points are used (see
Table~\ref{XXX} of the Supplementary Materials  \IK{include that}). For each sample, we
compute the MAL and MSPAL estimates, as well as the \texttt{bglmer} estimates based on
normal and t priors.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width = \textwidth]{Figures/simulation_results.pdf}
  \end{center}
  \caption{Performance metrics for parameter estimates of MAL,MSPAL and \texttt{bglmer} from simulating a Bernoulli-response GLMM from the Culcita data at the MAL}
  \label{fig:culcita_simu0}
\end{figure}

Figure~\ref{fig:culcita_simu0} shows boxplots for the sampling
distributions of the estimators, centred at the true value, the estimated
finite-sample bias, variance, mean squared error, and probability of
underestimation for each estimator, along with the estimated coverage
of 95\% Wald confidence intervals based on the estimates and estimated
standard errors from the negative Hessian of the approximate
log-likelihood at the estimates. The plotting range for the support of
the distributions has been restricted to $(-11, 11)$, which does not
contain all MAL estimates in the simulation study but contains all
estimates for the other methods. We should note here that apart from
the estimated probability of underestimation,
estimates for the other summaries are not well-defined for MAL,
because the probability of boundary estimates is positive. In fact,
there were issues with at least one of the MAL estimates for $9.25\%$
of the simulated samples. These issues are either due to convergence
failures or because the estimates or estimated standard errors have
been found to be atypically large in absolute value. The displayed
summaries for MAL are computed based only on estimates which have not
been found to be problematic. Clearly, the amount of shrinkage induced
by the normal and t priors is excessive. Although the resulting
estimators have small finite-sample variance (with the one based on
normal priors having the smallest), they have excessive finite-sample
bias, which is often at the order of the standard deviation resulting
in large mean squared errors, and the sampling distributions to be
located far from the respective true values. Importantly, the
combination of small variance and large bias readily impacts
first-order inferences; Wald-type confidence intervals about the fixed
effects are found to systematically undercover the true parameter
value. Finally, both bglmer(n) and bglmer(t) do not appear prevent extreme positive variance estimates.

As is apparent from Table~\ref{tab:culcita_inf}, the identities on the
model parameters hold exactly with the proposed MSPAL estimates, where the observed deviations are attributed to rounding errors. Furthermore, from
Figure~\ref{fig:culcita_simu0} we see that the penalty we propose not only ensures
that estimates are away from the boundary of the parameter space, but
its soft nature guarantees that estimators have the optimal
frequentist properties that would be expected by the MAL estimator
had it not taken boundary values. 


\section{Penalized likelihoods}
\label{sec:softpen}

\subsection{Setup}

Suppose that we observe the values $\by_1, \ldots, \by_k$ of a
sequence of random vectors $\bY_1, \ldots, \bY_k$ with
$\by_i = (y_{i1}, \ldots, y_{in_i})^\top \in \mathcal{Y} \subset
\Re^{n_i}$, possibly with a sequence of covariate vectors
$\bv_1, \ldots, \bv_k$, with
$\bv_i = (v_{i1}, \ldots, v_{is})^\top \in \mathcal{X} \subset
\Re^{s}$. Let $\bY = (\bY_1^\top, \ldots, \bY_k^\top)^\top$, and
denote by $\bV$ the set of $\bv_1, \ldots, \bv_k$. Further, assume that the data generating process of $\bY$, conditional on $\bV$ has a density or
probability mass function $f(\bY \mid \bV; \btheta)$, indexed by a
parameter $\btheta \in \Theta \subset \Re^d$. Denote the parameter that identifies the conditional distribution of $\bY$ given $\bV$ by $\btnod \in \Theta$. 

A popular method for estimating the parameter vector $\btnod$ is to
maximize the logarithm of the likelihood $f(\bY \mid \bV; \btheta)$ with respect to
$\btheta$. If the likelihood is not
available in closed form then an approximation of it may be maximized
instead. For example, expression~(\ref{eq:bern_likl}) gives
$f(\bY \mid \bV; \btheta)$ in the case of the Bernoulli-response GLMMs
of Section~\ref{sec:bern_GLMMs}, and in Section~\ref{sec:culcita_dat}
we use an adaptive Gauss-Hermite quadrature approximation to the
log-likelihood. In what follows, $\ell(\btheta)$ denotes either the
log-likelihood or an approximation to it whenever that distinction is
immaterial for the context. Furthermore, the dependence of
$\ell(\btheta)$ on $\bY$ and $\bV$ is suppressed for notational
convenience. Then, the ML (or MAL) estimator of $\btheta$ is defined
as $\hat\btheta = \arg \max_{\btheta\in \Theta} \ell(\btheta)$.

Let $\tilde\btheta$ be the the maximum penalized likelihood (MPL) (or
maximum penalized approximate likelihood; MPAL) estimator
\[
  \tilde\btheta = \arg\max_{\btheta \in \Theta} \{\ell(\btheta) + P(\btheta) \} \, ,
\]
where $P(\btheta)$ is an additive penalty to $\ell(\btheta)$ that may depend on $\bY$ and $\bV$. 

In the remainder of this section we derive the conditions that
$P(\btheta)$ must satisfy to ensure that the MPL or MPAL
estimator $\tilde\btheta$ i) takes values always in the interior of
$\Theta$, ii) is invariant under linear transformations of the
parameters, such as scaled contrasts that are usually employed with
categorical covariates in regression modelling, and iii) has similar first order asymptotics to $\hat{\btheta}$. We then derive a penalty that satisfies
those conditions for Bernoulli-response GLMMs.

\subsection{Interior point parameter estimates}
\label{sec:interior}
Denote by $\partial \Theta$ the boundary of the parameter space, with respect to the $d$-fold Cartesian product of the extended real line $\Re \cup \{-\infty, \infty\}$ (to capture the notion that parameter estimates with infinite components lie on the boundary of the parameter space.
Let $\btheta(r)$, $r \in \Re$, be a path in the parameter space such
that $\lim_{r \to \infty}\btheta(r) \in \partial \Theta$. A common
approach to resolving issues with ML or MAL estimates being in
$\partial \Theta$, like those encountered in the example of
Section~\ref{sec:culcita_dat}, is to instead use MPL or MPAL
estimators from a penalty that satisfies
$\lim_{r \to \infty} P(\btheta(r)) = -\infty$ and which are bounded from above. Then, if there is at
least one point $\btheta \in \Theta$ such that
$\ell(\btheta)  > -\infty$, it must hold that
$\tilde\btheta$ is in the interior of $\Theta$.

For example, the penalties arising from the independent normal and
independent t prior structures implemented in \texttt{blme} are such
that $\lim_{r \to \infty} P(\btheta(r)) = -\infty$, whenever
$\btheta(r)$ diverges to the boundary of the parameter space for the
fixed effects. As a result, the bglmer(n) and bglmer(t) estimates for
the fixed effects in Table~\ref{tab:culcita_inf}) are finite. On the
other hand, the default gamma-prior like penalty used in
\texttt{bglmer} for the variance component $\sigma$ is
$-1.5 \log\sigma$, which, while it ensures that the estimate of
$\log \sigma$ is not minus infinity, does not guard from positive
infinite estimates. This is apparent in
Figure~\ref{fig:culcita_simu0}, where several extreme positive
bglmer(n) and bglmer(t) estimates are observed for $\log\sigma$; see,
also, the vignettes of the \texttt{glmmsr} \citep{ogden:2019} R package
for an example with infinite variance component estimate in a
Bernoulli-response GLMM.

\subsection{Invariance under scaled linear transformations}

The ML estimates are known to be invariant to transformations of the
model parameters \citep[see, for example][]{zehna:1966}. A
particularly useful class of transformations in regression modelling
with categorical covariates is the collection of scaled linear transformations
$\btheta' = \bC \btheta$ for known, invertible, real matrices
$\bC$. With such transformations one can obtain ML or MAL estimates and corresponding
estimated standard errors for arbitrary sets of scaled parameter
contrasts, when estimates for one of those sets of contrasts are
available and with no need to re-estimate the model. Further, these transformations eliminate estimation and
inferential ambiguity when two independent researchers analyse the
same data set using the same model but with different contrasts,
e.g. due to software defaults.

The example in Section~\ref{sec:culcita_dat} shows that not all MPL or
MPAL estimators are invariant to linear transformations of the
parameters. The condition required for achieving invariance is that
the penalty satisfies $P(\bC\btheta) = P(\btheta) + b$, where
$b \in \Re$ is a real constant. This requirement does not hold for
the penalties arising from the normal and t prior structures that are
used to compute the bglmer(n) and bglmer(t) fixed effect estimates in Table~\ref{tab:culcita_inf}. Hence, the bglmer(n) and
bglmer(t) MPAL estimates are not invariant under linear
transformations of the parameters.

\subsection{Asymptotic properties}
\label{sec:ass+res}

Consistency, asymptotic normality and valid asymptotic hypothesis testing of the proposed MSPAL estimator follow readily from similar such results for MAL estimators where the approximation error to the model log-likelihood is an additive error term. Indeed, the results presented in this section are a direct translation of the work of \citet{ogden:2017}, where the term ``approximation error'' is replaced by ``penalty function''. To state the results and their underlying assumptions, we introduce some further notation. Proofs are given in Section S2 of the supplementary material.

Let $S(\btheta) $ be the score function of $\ell(\btheta)$, i.e. $S(\btheta ) =\nabla \ell(\btheta)$, and let $\tilde{S}(\btheta) = \nabla\ell(\btheta) + \nabla P(\btheta)$ be the score of its penalized analogue $\tilde{\ell}(\btheta) = \ell(\btheta)+P(\btheta)$. Denote the observed information matrix by $J(\btheta) = -\nabla \nabla^\top \ell(\btheta)$. It is assumed that information regarding the model parameter accumulates at a rate $r_n$ in the sense that $r_n^{-1}J(\btheta ) \overset{p}{\to} I(\btheta)$ as $n \to \infty$ for some nonrandom, positive definite, $\mathcal{O}(1)$, matrix $I(\btheta)$ and with respect to some matrix norm $\mnorm{\cdot}$. Further, let $\delta(\btheta) = \vnorm{\nabla P(\btheta)}$ % and $\gamma(\theta)=\norm{ J(\theta)-\tilde{J}(\theta)}$ 
for some vector norm $\vnorm{\cdot}$, and for $S \subseteq \Theta$ define $\delta^\infty(S)  = \underset{\btheta \in S}{\sup} \; \delta(\btheta)$ and $\delta^\infty = \delta^\infty(\Theta)$. Finally, denote by $B_t(\btheta)$ the ball of radius $t$ around $\btheta$. 

We impose standard M-estimation regularity conditions on the score function to establish consistency of $\bttilde$ (see for example \citet[Chapter 5]{vaart:1998}). 
	\begin{itemize}
	\item[A0] Both $\ell(\btheta),\tilde{\ell}(\btheta)$ are differentiable, with  derivatives $S(\btheta),\tilde{S}(\btheta)$ 
	\item[A1] $\underset{\btheta \in \Theta}{\sup} \; \vnorm{r_n^{-1} S(\btheta) - S_0(\btheta)} \overset{p}{\to}0$ for some deterministic function $S_0(\btheta)$ 
	\item[A2] For all $\varepsilon>0$, $\underset{\btheta \in \Theta: \vnorm{\btheta-\btnod}\geq \varepsilon}{\inf} \vnorm{S_0(\btheta) }>0 = \vnorm{S_0(\btnod)}$ 
	\item[A3] $\hat{\btheta}$ and $\bttilde$ are roots of $S(\btheta),\tilde{S}(\bttilde)$, i.e. $S(\hat{\btheta}) = \b0$ and $\tilde{S}(\bttilde) = \b0$
\end{itemize}
\begin{theorem}[Consistency]
	\label{thm:soft_pen_cons}
	Let $\delta^\infty = o_p(r_n)$, and assume that A0-A3 hold. Then $\bttilde \overset{p}{\to} \btnod$.  
\end{theorem}
The regularity conditions we impose to establish asymptotic normality of $\bttilde$ are standard conditions in maximum likelihood estimation. 
\begin{itemize}
	\item[A4] Both $\ell(\btheta),\tilde{\ell}(\btheta)$ are three times differentiable
	\item[A5] $\underset{\btheta \in \Theta}{\sup}\; \mnorm{ r_n^{-1}J(\btheta) -I(\btheta) } \overset{p}{\to} 0$ for some positive definite, nonrandom, $\mathcal{O}(1)$ matrix $I(\btheta)$, that is continuous in $\btheta$ in a neighbourhood around $\btnod$
	\item[A6] $r_n^{1/2}(\hat\btheta-\btnod) \overset{d}{\to} \text{N}(0,I(\btnod)^{-1})$
	\item[A7] $\bttilde$ is consistent for $\btnod$
\end{itemize}
\begin{theorem}[Asymptotic Normality]
	\label{thm:asymp_norm_soft_pen}
	Assume that conditions A3-A7 hold. Let $\delta^\infty =   o_p(r_n)$ and assume there is a $t>0$ such that $\delta^{\infty}(B_t(\btnod)) = o_p(r_n^{1/2})$. Then 
	$r_n^{1/2}(\bttilde-\btnod) \overset{d}{\to} \text{N}(0,I(\btnod)^{-1})$. 
\end{theorem}  


To state conditions for valid hypothesis testing using the MSPAL, let $\gamma^\infty(S) = \underset{\btheta \in S}{\sup} \mnorms{\nabla \nabla^\top P(\btheta)}$ and suppose we want to test $H_0: \btheta \in \Theta^R$, where $\Theta^R \subset \Theta$ and $\dim(\Theta^R) < \dim(\Theta)$. Finally, let $\Lambda = 2(\ell(\hat \btheta) - \ell(\hat \btheta^R) )$ and similarly $\tilde{\Lambda} = 2( \tilde{\ell}(\tilde \btheta) - \tilde{\ell}(\tilde \btheta^R) )$, where $\hat \btheta^R,\tilde \btheta^R$ denote the maximizers of $\ell(\btheta),\tilde{\ell}(\btheta)$ over $\Theta^R$ respectively.
 
\begin{theorem}[Hypothesis testing]\label{thm:hypo}
	Assume that conditions A3-A7 hold and that $\delta^\infty = o_p(r_n)$, $\delta^\infty(B_t(\btnod)) = o_p(r_n^{1/2})$ and $\gamma^\infty(B_t(\btnod)) = o_p(r_n)$ for some $t>0$. Then, under $H_0: \tilde{\Lambda} -~\Lambda =~o_p(1)$.
\end{theorem}
The conditions of Theorems \ref{thm:soft_pen_cons}-\ref{thm:hypo} are one of many and other standard arguments to establish consistency and asymptotic of a maximum likelihood estimator are expected to lead to the same results. We note that the large sample results of the MSPAL operate under the assumption that $\ell(\btheta)$ is the exact model likelihood or that $\ell(\btheta)$ is an approximate likelihood for which the convergence and regularity assumptions of A0-A7 are with respect to a quantity of interest. Corollaries S2.1-S2.3 of the supplementary material give sufficient conditions about the approximation error to achieve the asymptotic results of Theorems \ref{thm:soft_pen_cons}-\ref{thm:hypo} with an approximate likelihood. It is left to future research to link the approximation error rates of various approximation methods with these conditions. There are results on approximation errors of the log-likelihhod, that can be adapted to match our conditions. We refer the reader to \citet{ogden:2021} for approximation errors to the log-likelihood in clustered GLMMs using Laplace's method, \citet{ogden:2017} for approximation errors to the gradient of the log-likelihood with an example for an intercept-only Bernoulli-response GLMM, \citet{stringer:2022} for approximation errors to the log-likelihood in clustered GLMMs using Adaptive Gauss-Hermite quadrature and \citet{jin+andersson:2020} for general approximation errors for adaptive Gauss-Hermite quadrature. 


\subsection{Soft penalization}
\label{sec:soft_pen}
The conditions that we imposed on the penalty function for the asymptotic results of $\bttilde$, namely $\delta^\infty = o_p(r_n)$ for consistency, and additionally $\delta^\infty(B_t(\btnod)) = o_p(r_n^{1/2})$ for some $t>0$ for asymptotic normality and $\gamma^\infty(B_t(\btnod)) = o_p(r_n)$ for hypothesis testing, can be decomposed into a (uniform) boundedness condition on the gradient and Hessian of the penalty and a rate requirement. Hence, the following blueprint provides a straightforward way of constructing appropriate penalty functions. i) Find an unscaled penalty function $P_u(\btheta)$ that guarantees estimates in the interior of the parameter space (see Section \ref{sec:interior}), ii) determine uniform bounds of $\vnorm{\nabla P_u(\btheta)}$, $\mnorm{\nabla \nabla^\top P_u(\btheta)}$ over $\Theta$, and iii) rescale the penalty function in dependence of $r_n$ to meet the rate requirements of Theorems \ref{thm:soft_pen_cons}-\ref{thm:hypo}. Note that the normal and t priors as well as the gamma and wishart priors that \texttt{bglmer} uses to penalize the fixed effects and variance components of a GLMM are not directly applicable in this framework as they do not have uniformly bounded gradients.
\section{Softly-penalized likelihood for Bernoulli-response GLMMs}
\label{sec:glmm_penalties}

\subsection{Fixed effects penalty}
\label{sec:glmm_fe_pen}
The unscaled fixed effects penalty we consider in this paper is the logarithm of Jeffreys invariant prior from a logistic GLM, that is $$P_u^{\textit{FE}}(\bbeta) =  \frac{1}{2}\log \det(\bX^\top \bW \bX).$$ Here $\bX$ is the matrix of all fixed effect covariates, $\bW$ is a diagonal matrix with diagonal entries $\bW_{ii} = \mu_i(\bbeta) (1-\mu_i(\bbeta))$ and $\mu_i(\bbeta)$ is the inverse logit-transform of the fixed effects component of the linear predictor at a point $\bbeta$ in the parameter space. For notational convenience, the dependence of $\mu(\bbeta)$ on $\bbeta$ is henceforth suppressed. \citet[Theorem 1]{kosmidis+firth:2020} have shown that whenever $\bX$ is full rank, then for any path $\bbeta(r) \in \Re^p$ indexed by $r \in \Re$ such that $\lim\limits_{r \to \infty} \bbeta(r) = \bbeta^\infty$, where $\bbeta^\infty$ is an arbitrary point in $\Re^p$ with at least one infinite component, $\lim\limits_{r \to \infty} \det(\bX^\top\bW\bX) = 0$. Therefore, noting that \eqref{eq:bern_likl} is always bounded from above by one as the conditional distribution of the response is a probability mass function, and that $\log \det (\bX^\top \bW \bX)$ is nonzero for $\bbeta=\b0_p$ when $\bX$ has full rank, adding this penalty to the log-likelihood guarantees finite fixed effect parameter estimates as long as there is one $\btheta \in \Theta$ for which the log-likelihood is not $-\infty$. \citet{kosmidis+firth:2020} show further, that Jeffreys invariant prior guarantees finite fixed effects estimates for other link functions, such as the probit, complementary log-log, log-log and cauchit link, so that the proposed penalty can be generalized to GLMMs where other link functions appear more natural. 

The bounds on the first and second order partial derivatives of Jeffreys invariant prior in \eqref{eq:jeffrey_deriv_bound} and \eqref{eq:jeffrey_deriv_bound2}, can be used to establish the range of scaling factors that are in line with Theorems \ref{thm:soft_pen_cons}- \ref{thm:hypo}. In particular, we show in Theorem S3.1 of the supplementary material, that for any full rank matrix $\bX \in \Re^{n \times p}$ and any $\bbeta \in \Re^p$ it holds that 
\begin{align}\label{eq:jeffrey_deriv_bound}
\left|\frac{\partial }{\partial \beta_i}\log \textrm{det}(\bX^\top\bW\bX)\right| & \leq p\underset{1\leq j\leq n}{\max} |x_{ji}| \\ \label{eq:jeffrey_deriv_bound2}
\left|\frac{\partial^2}{\partial \beta_i \partial \beta_j} \log\det (\bX^\top \bW \bX) \right| &\leq 2p\underset{1\leq k \leq n}{\max} \; \left|x_{ki}\right| \underset{1\leq k \leq n}{\max} \; \left|x_{kj}\right|  
\end{align}
Hence, as long as $r_n$ is increasing $n$, any scaling factor that is $\Op{\underset{i,j}{\max} \; |x_{ji}|^{-1}}$ achieves appropriate scaling of Jeffreys invariant prior for consistency and asymptotic normality and any scaling that is $\Op{\underset{i,j}{\max} \; |x_{ji}|^{-2}}$ achieves valid asymptotic hypothesis testing. 

 We propose scaling Jeffreys invariant prior by $ 2 \sqrt{p/n} $, which gives the scaled fixed effects penalty 
\begin{equation}\label{eq:scaled_jeffreys}
	P^{\textit{FE}}(\bbeta) = \sqrt{p/n} \log \det(\bX^\top \bW \bX)
\end{equation}
By \eqref{eq:jeffrey_deriv_bound} and \eqref{eq:jeffrey_deriv_bound2}, it then follows that \eqref{eq:scaled_jeffreys} is a valid penalty whenever $\max_{i,j} |x_{ji} | = \Op{n^{1/2}}$ as long as $r_n$ is increasing in $n$. This certainly holds for bounded covariates, as considered in our real-data examples, as well as, for example, for covariate matrices whose entries are subgaussian random variables with common variance proxy $\sigma^2$, in which case $ \underset{i,j}{\max}\;  |x_{ji} | = \Op{\sqrt{2\sigma^2 \log(2np)}}$ (see for example \citet[Theorem 1.14]{rigollet:2015}). 

\subsection{Variance components penalty}
\label{sec:glmm_re_pen}
The variance components penalty we propose in this paper is the negative Huber loss function, and a multivariate generalization thereof, that is scaled appropriately to ensure asymptotic negligibility in line with Theorems \ref{thm:soft_pen_cons}-\ref{thm:hypo}. 

We first consider the case univariate random effects, for which we propose to penalize $\log\sigma$ by the negative Huber loss with $\delta$-parameter equal to one, that is 
\begin{equation} \label{eq:huber}
P_u^{\textit{RE}}(\log\sigma) = \begin{cases}
-\frac{1}{2} \{\log \sigma\}^2, & \text{if } |\log \sigma|\leq 1 \\ 
- |\log \sigma| + \frac{1}{2}, & \text{otherwise}
\end{cases}
\end{equation}
Following the discussion of Section \ref{sec:soft_pen}, the variance components penalty of \eqref{eq:huber} must satisfy $\lim\limits_{\sigma \to 0} \; P_u^{\text{RE}}(\sigma) = -\infty$ and $\sup_{\sigma \in (0,\infty) } \vnorm{\nabla P_u^{\text{RE}}(\sigma)}$ must be bounded. Note however that the domain of $P^{\text{RE}}_u(\sigma)$, is bounded from below, so that if a penalty function $P^{\text{RE}}_u(\sigma):\Re_{>0} \to \Re$ is differentiable with uniformly bounded derivative over its domain, then it cannot be that $\lim\limits_{\sigma \to 0}P^{\text{RE}}_u(x) = -\infty$. In the absence of a uniform bound on the gradient of the variance components penalty, it is not possible to apply the developed methodology to a penalty on the random effects variance parameter $\sigma$ directly. A workaround is to parametrize the model in terms of $\log\sigma$, the range of which is $\Re$, rather than $\sigma$ itself. For this reparametrized model, it is easily verified that the Huber loss of \eqref{eq:huber} has uniformly bounded first and second derivatives. Naturally, this implies that assumptions A0-A7 must apply to the reparametrized model. The continuous mapping theorem (see for example  \citet[Theorem. 2.3]{vaart:1998}) and the delta method (see for example \citet[Chapter 3]{vaart:1998}) provide asymptotic results for the $\sigma$ parametrization. 

We propose scaling the negative Huber loss penalty by $2\sqrt{p/n}$ yielding the random effects penalty 
\begin{equation} \label{eq:scaled_huber}
P^{\textit{RE}}(\log\sigma) = \sqrt{p/n}\begin{cases}
- \{\log \sigma\}^2, & \text{if } |\log \sigma|\leq 1 \\ 
- 2|\log \sigma| + 1, & \text{otherwise}
\end{cases}
\end{equation}

The negative Huber loss penalty on the log-transformed random effects variance can easily be extended to multivariate random effects. For this, we consider the Cholesky factorization, call it $\bL$, of the variance components matrix $\bSigma = \bL\bL^\top$. Since for positive definite matrices, the map from $\bSigma$ to $\bL$ is bijective, this reparametrization is well defined. To ensure that the diagonal entries of $\bL$ are finite and positive, we penalize the logarithm of each main-diagonal entry by \eqref{eq:scaled_huber}. To ensure finiteness of all lower-triangular entries off the main-diagonal, each entry is again penalized by the same penalty without the prior log-transform. This ensures that the resulting variance-covariance estimate, $\widetilde{\bSigma} = \bLt\bLt^\top$, where all main-diagonal entries are transformed back to their natural parametrization, is nondegenerate. That is to say, $\bSigma$ is symmetric, positive definite, with finite entries and exhibits no perfect estimated correlation, i.e. for all $i \neq j$, $\left|\frac{\widetilde{\bSigma}_{ij} }{\sqrt{ \widetilde{\bSigma}_{ii} \widetilde{\bSigma}_{jj}}}\right|<1$. A proof is given in Lemma S4.1 of the supplementary material. Again, we require that all model regularity assumptions apply with respect to log-transformed diagonal entries of $\bL$, rather than $\bL$. Large sample theory for $\widetilde{\bSigma}=\bLt\bLt^\top$ follows from the continuous mapping theorem and the delta method. 

The Theorem below establishes that our proposed penalties give estimates in the interior of the parameter space for a Bernoulli-response GLMM. A proof is given in Section 4.2 of the supplementary material. 

\begin{theorem}[Interior point estimates]\label{thm:int_point}
	Let $\ell(\bb \theta)$ be the log-likelihood of Bernoulli-response GLMM, where $\bb \theta = (\bb \beta,  \bb L  )$ and $\bb L$ is the Cholesky factor of the variance components matrix $\bSigma$. Let 
	\begin{equation}
	\label{eq:pen_max}
	\tilde\btheta = (\tilde{\bb \beta},\tilde{\bb L}) = \arg\max_{\btheta \in \Theta} \{\ell(\btheta) + P^{\text{FE}}(\bb \beta) + P^{\text{MVRE}}(\bL) \} \, ,
	\end{equation}
	be the maximizer of the penalized model log-likelihood, with 
	\begin{equation}
	P^{\text{MVRE}}(\bL) = \sum_{i=1}^{N_q}P^{\text{RE}}(\log(l_{ii})) + \sum_{i<j}^{q}P^{\text{RE}}(l_{ij}),
	\end{equation}
	\begin{equation}
	P^{\text{RE}}(x) \propto \begin{cases}
	-\frac{1}{2} \{x \}^2, & \text{if } |x|\leq 1 \\ 
	- |x| + \frac{1}{2}, & \text{otherwise}
	\end{cases}  ,
	\end{equation}
	and
	\begin{equation} 
	P^{\text{FE}}(\bb \beta) \propto \log \det(\bX^\top\bW \bX) \, .
	\end{equation}
	Then, if $\bttilde$ exists, $\tilde{\bb \Sigma} = \tilde{\bb L} \tilde{\bb L}^\top$ is nondegenerate and all components of $\tilde{\bb \beta}$ are finite whenever there is a $\bb \theta$ in the interior of $\Theta$ such that $\ell(\bb \theta) >-\infty$.
\end{theorem}

\section{Example: conditional inference data} 
\label{sec:ci}
To demonstrate the performance of the MSPAL on a Bernoulli-response GLMM with multivariate random effects structure, we consider a subset of the data analysed by \citet{singmann+etal:2016}. As discussed on CrossValidated (\url{https://stats.stackexchange.com/questions/38493}), this data set exhibits both infinite fixed effects estimates as well as degenerate variance components estimates when a Bernoulli-response GLMM is fitted by MAL. 

The data set, originally collected as a control condition of experiment 3)b) in \citet{singmann+etal:2016} and therein analysed in a different context, comes from an experiment in which participants worked on a probabilistic conditional inference task. Participants were presented with the conditional inferences modus ponens (MP), modus tollens (MT), affirmation of the consequent (AC), and denial of the antecedent (DA), for four conditional rules with varying degrees of counterexamples (alternatives, disablers) that are listed below.
\begin{enumerate}
	\item If a predator is hungry, then it will search for prey. (few disablers, few alternatives)
	\item If a person drinks a lot of coke, then the person will gain weight. (many disablers, many alternatives)
	\item If a girl has sexual intercourse with her partner, then she will get pregnant. (many disablers, few alternatives)
	\item If a balloon is pricked with a needle, then it will quickly loose air. (few disablers, many alternatives)
\end{enumerate}
For each conditional rule and inference, participants were asked to estimate the probability that the conclusion follows from the conditional rule given the minor premise. For example, if MP is ``\textit{If p then q. p.}'', participants were asked ``\textit{If p then q. p. How likely is q?}''. Additionally, participants were asked to estimate the probability of the premises themselves. The response variable of this dataset is then a binary response indicating whether, given a certain conditional rule and inference, the participants' probabilistic inference is p-valid; that is, whether their estimate of uncertainty about the conclusion does not exceed the estimated uncertainty of the premises (p-valid inferences are recorded as zero, p-invalid inferences as one). Covariates are the categorical variable counterexamples (``many'', ``few''), that indicates the degree of available counterexamples to a conditional rule, type (``affirmative'',``denial'') which describes the type of inference (MP and AC are affirmative, MT and DA are denial), and p-validity (``valid'',``invalid''), indicating whether an inference is p-valid per se (MP and MP are p-valid, while AC and DA are not). For each of the 29 participants, there exist 16 observations corresponding to all possible combinations of inference and conditional rule, giving a total of 464 data points, which are grouped along individuals by the clustering variable code. We can employ a Bernoulli-response GLMM to investigate the probabilistic validity of conditional inference given the type of inference and conditional rule as captured by the covariates and all possible interactions thereof. We introduce a random intercept and random slope for the variable counterexamples to account for response heterogeneity between participants. Hence the model we are considering is given by    
\begin{align}
\label{eq:cond_inf_model} 
  Y_{ij} \mid \bb{u}_i & \sim \text{Bernoulli}(\mu_{ij}) \quad \text{with} \quad
g(\mu_{ij}) = \eta_{ij} = \bx_{ij}^\top \bbeta + \bz_{ij}^\top \bu_i\\
\bu_i & \sim \text{N}(\b0_2, \bb{\Sigma})  \quad (i = 1, \ldots, 29; j = 1, \ldots, 16)\,,
\end{align}
where $\bbeta = (\beta_0,\beta_1,\ldots,\beta_8)$ are the fixed effects pertaining to the model matrix of the R model formula \texttt{response \raisebox{-0.9ex}{\~{}} type * p.validity * counterexamples + (1+counterexamples|code)}. As (adaptive) Gauss-Hermite quadrature becomes computationally challenging and not available for \texttt{glmer} and consequently \texttt{bglmer} for multivariate random effect structures, we approximate the likelihood of model \eqref{eq:cond_inf_model} about the parameters $\bbeta$, $\bL$ using Laplace's method (see for example \cite{pinheiro+bates:1995}). We estimate the parameters $\bbeta$, $\bL$ by MAL using the optimization routines ``CG'' (``MAL(CG)'') and ``BFGS'' (``MAL(BFGS)'') of the \texttt{optimx} R package \citep{nash+varadhan:2011}, \texttt{bglmer} from the \texttt{blme} R package \cite{chung+etal:2013} using independent normal (``bglmer(n)'') and t (``bglmer(t)'') priors for the fixed effects and the default wishart prior for the multivariate variance components. We also estimate the parameters using the proposed MSPAL estimator with the fixed and random effects penalties of Sections \ref{sec:glmm_fe_pen} - \ref{sec:glmm_re_pen}. The estimates are given in Table \ref{tab:cond_inf}, where we denote the entries of $\bL$ by $l_{ij}$, for $i,j=1,2$. 
\begin{table}[H]
	\centering
	\caption{Estimates from the conditional inference dataset of \citet{singmann+etal:2016} using MAL, \texttt{bglmer} and MSPAL}
	\label{tab:cond_inf}
	\centering
	\begin{tabular}{lD{.}{.}{3}D{.}{.}{3}D{.}{.}{3}D{.}{.}{3}D{.}{.}{3}}
		\toprule
		&
		\multicolumn{1}{c}{MAL(BFGS)} & 
		\multicolumn{1}{c}{MAL(CG)} &
		\multicolumn{1}{c}{bglmer(t)} &
		\multicolumn{1}{c}{bglmer(n)} & 
		\multicolumn{1}{c}{MSPAL} \\
		\midrule
$\beta_0$ & 16.25 & 7.73 & 13.22 & 5.45 & 6.22 \\ 
& (2.57) & (4.00) & (1.63) & (8.15) & (2.89) \\ 
$\beta_2$ & 4.23 & 3.33 & 1.86 & 0.97 & 0.00 \\ 
& (1.19) & (14.44) & (3.01) & (2.98) & (4.08) \\ 
$\beta_3$ & -6.69 & -2.08 & -0.09 & -0.13 & -2.17 \\ 
& (1.77) & (2.98) & (1.77) & (2.43) & (2.98) \\ 
$\beta_4$ & -14.40 & -5.96 & -11.04 & -2.88 & -4.37 \\ 
& (2.58) & (4.03) & (1.90) & (8.99) & (2.91) \\ 
$\beta_5$ & 3.17 & 0.85 & 0.47 & 0.34 & 2.17 \\ 
& (1.36) & (16.40) & (4.54) & (4.32) & (5.02) \\ 
$\beta_6$ & -4.23 & -3.20 & -1.98 & -1.03 & 0.00 \\ 
& (1.19) & (14.45) & (3.05) & (3.04) & (4.11) \\ 
$\beta_7$ & 8.19 & 3.81 & 1.44 & 1.39 & 3.64 \\ 
& (1.83) & (3.11) & (1.94) & (2.56) & (3.09) \\ 
$\beta_8$ & -3.90 & -1.86 & -1.00 & -0.80 & -2.87 \\ 
& (1.91) & (16.43) & (4.66) & (4.44) & (5.12) \\ 
$\log l_{11}$ & 2.02 & 0.81 & 4.52 & 4.52 & -0.63 \\ 
& (0.36) & (1.14) & (0.01) & (0.01) & (2.48) \\ 
$l_{21}$ & -7.70 & -2.43 & -91.89 & -92.97 & -0.60 \\ 
& (2.45) & (2.58) & (0.25) & (0.45) & (1.69) \\ 
$\log l_{22}$ & -5.16 & -2.94 & -0.27 & -0.58 & -1.21 \\ 
& (82.47) & (8.77) & (0.53) & (0.84) & (1.30) \\ 
		\bottomrule
	\end{tabular}
\end{table}
As in the Culcita example of Section \ref{sec:culcita_dat}, we encounter fixed effects estimates that are extreme on the logistic scale for both MAL(BFGS), MAL(CG) and bglmer(t). We further note that the strongly negative estimates for $l_{22}$ in conjunction with the inflated asymptotic standard errors of the MAL(BFGS) estimates are highly indicative of parameter estimates on the boundary of the parameter space, meaning that $l_{22}$ is essentially estimated as zero. The degeneracy of the variance components estimates is even more striking for the estimates using \texttt{bglmer}, which give estimates of $l_{11},l_{21}$ greater than $90$ in absolute value, which corresponds to estimated variance components greater than $8000$ in absolute value. This underlines that, as with the gamma prior penalty for univariate random effects, the wishart prior penalty, while effective in preventing variance components being estimated as zero, cannot guard against infinite estimates for the variance components. We finally note that for the MSPAL, all parameter estimates as well as their estimated standard errors appear to be finite. Further, while the variance components penalty guards against estimates that are effectively zero, the penalty induced shrinkage towards zero is not as strong as with the whishart prior penalty of the \texttt{bglmer} function. To further investigate the frequentist properties of the estimators on this dataset, we repeat the simulation design of the Culcita data example from Section \ref{sec:culcita_dat} for the conditional inference data where we set the MSPAL estimate of Table \ref{tab:cond_inf} as the ground truth. We point out the extremely low percentage of \texttt{bglmer} estimates without estimation issues that were used in the summary of Figure \ref{fig:cond_inf_simul}. While for MSPAL, over 99\% of estimates were used in the calculation of the summary statistics of Figure \ref{fig:cond_inf_simul}, less than 6\% were used for the  \texttt{bglmer} methods. We note that the MSPAL, which is the only estimation method that is guaranteed to give nondegenerate variance components estimates, outperforms MAL and \texttt{bglmer}, which incur substantial bias and variance due to their singular and infinite estimates of variance components. Table \ref{tab:cond_inf} shows the percentiles of the centered estimates for each estimation method, and underlines that MAL and \texttt{blgmer} are unable to guard against degenerate variance components estimates. 

\begin{figure}[H]
	\begin{center}
		\includegraphics[width=\textwidth]{Figures/cond_inf_simul.pdf}
	\end{center}
	\caption{Performance metrics for variance components estimates of MAL,MSPAL and \texttt{bglmer} from simulating a Bernoulli-response GLMM from the conditional inference data at the MSPAL}
	\label{fig:cond_inf_simul}
\end{figure}

\begin{table}[H]
	\centering
	\caption{Percentiles of centered variance components estimates from simulating a Bernoulli-response GLMM from the conditional inference data at the MSPAL} 
	\label{tab:sim2}
	\begin{tabular}{lrcccccccc}
		\toprule 
		&& \multicolumn{6}{c}{Percentiles} \\ \cmidrule{3-9}
		 &  & 5\% & 10\% & 25\% & 50\% & 75\% &90\% & 95\% \\  
		\cmidrule{3-9}
		& $\log l_{1,1}$ & -0.06 & -0.05 & -0.02 & 0.01 & 0.13 & 0.46 & 0.68 \\ 
		MSPAL & $\log l_{2,2}$ & -0.44 & -0.32 & -0.10 & 0.25 & 0.55 & 0.84 & 0.98 \\ 
		& $l_{2,1}$ & -0.71 & -0.26 & -0.02 & 0.12 & 0.21 & 0.34 & 0.42 \\ \cmidrule{3-9}
		& $\log l_{1,1}$ & 1.03 & 1.09 & 1.30 & 1.79 & 3.89 & 4.64 & 4.95 \\ 
		bglmer(t) & $\log l_{2,2}$ & 0.43 & 0.53 & 0.65 & 0.97 & 1.11 & 1.40 & 1.55 \\ 
		& $l_{2,1}$ & -41.35 & -34.26 & -6.34 & -2.67 & -1.20 & -0.79 & -0.69 \\ \cmidrule{3-9}
		& $\log l_{1,1}$ & 1.24 & 1.36 & 1.79 & 2.03 & 4.51 & 4.91 & 5.12 \\ 
		bglmer(n) & $\log l_{2,2}$ & 0.55 & 0.59 & 0.81 & 1.01 & 1.17 & 1.38 & 1.49 \\ 
		& $l_{2,1}$ & -39.80 & -32.07 & -25.08 & -3.06 & -2.59 & -2.47 & -2.09 \\ \cmidrule{3-9}
		& $\log l_{1,1}$ & -2.48 & -1.80 & -0.40 & 1.24 & 2.36 & 2.61 & 2.66 \\ 
		MAL & $\log l_{2,2}$ & -7.73 & -4.51 & -3.40 & -1.71 & 0.28 & 0.73 & 1.01 \\ 
		& $l_{2,1}$ & -7.49 & -7.19 & -5.95 & -2.45 & 0.52 & 0.80 & 1.06 \\ 
		\bottomrule
	\end{tabular}
\end{table}

\section{Discussion}
\label{sec:sum}
This paper proposed the MSPAL estimator for stable parameter estimation in Bernoulli-response GLMMs. We showed that using a scaled version of Jeffreys prior as fixed effects penalty and the negative Huber loss function as a variance components penalty gives nondegenerate estimates whose finite sample properties are superior to the penalized estimator proposed by \citet{chung+etal:2013}. While particularly relevant for Bernoulli-response GLMMs, the concept of MSPAL is far more general and we expect it to be useful in other settings, such as GLMMs with Binomial or Poisson responses, for which degenerate M(A)L estimates are known to occur. We leave deriving a unified set of conditions and error rates that satisfy the regularity assumptions that we imposed to derive asymptotic properties of the MSPAL to future research. 

%Finally, it is worth pointing out that throughout this study it was assumed that the model was correctly specified, meaning that the true model is contained within the class of models over which estimation is performed. We consider extending the framework of \citet{yu2018asymptotic} regarding model selection in GLMMs under misspecification to our proposed MSPAL so as to develop a softly penalized MAL estimator that is robust under model misspecification a promising avenue for future research.

\bibliographystyle{chicago}
\bibliography{softpen}
%\input{softpen.bbl} 
\includepdf[pages=-]{softpen_supplementary.pdf}
\end{document}

